from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id, udf, count, date_format
from pyspark.sql.types import StringType, TimestampType
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF
import nltk
from nltk import ne_chunk, pos_tag, word_tokenize
from nltk.tree import Tree
import json


#âœ… ExplicaciÃ³n de Mejoras en el CÃ³digo
#ğŸ”¹ AnÃ¡lisis de Contenido
#âœ” ExtracciÃ³n de palabras clave con TF-IDF
#âœ” IdentificaciÃ³n de entidades (compaÃ±Ã­as, personas, lugares) usando NLTK
#âœ” ClasificaciÃ³n de artÃ­culos por tema con palabras clave

#ğŸ”¹ AnÃ¡lisis de Tendencias
#âœ” DetecciÃ³n de tendencias en el tiempo con agrupaciÃ³n por fecha
#âœ” IdentificaciÃ³n de fuentes de noticias mÃ¡s activas


# ğŸ“Œ 1ï¸âƒ£ Inicia sesion de Spark
spark = SparkSession.builder \
    .appName("AnalisisNoticias") \
    .getOrCreate()

# ğŸ“Œ 2ï¸âƒ£ ConfiguraciÃ³n de Variables
BUCKET_NAME = "us-central1-flujotransacion-9cfbfa36-bucket"
RAW_PATH = f"gs://{BUCKET_NAME}/datos_raw/articles.json"
PROCESSED_PATH = f"gs://{BUCKET_NAME}/datos_procesados/articles_cleaned.parquet"
BIGQUERY_TABLE = "analitica-contact-center-dev.Entorno_Pruebas_modelo.noticias_procesadas"

# ğŸ“Œ 3ï¸âƒ£ Cargar Datos desde Cloud Storage
df = spark.read.json(RAW_PATH)

# ğŸ“Œ 4ï¸âƒ£ Limpieza de Datos
df_cleaned = df.dropDuplicates(["id"]) \
               .filter(col("title").isNotNull()) \
               .withColumnRenamed("news_site", "fuente") \
               .withColumnRenamed("published_at", "fecha_publicacion")

# ğŸ“Œ 5ï¸âƒ£ Convertir `published_at` a TIMESTAMP
df_cleaned = df_cleaned.withColumn("fecha_publicacion", col("fecha_publicacion").cast(TimestampType()))

# ğŸ“Œ 6ï¸âƒ£ Agregar ID Ãšnico
df_cleaned = df_cleaned.withColumn("id_articulo", monotonically_increasing_id())

# ğŸ“Œ 7ï¸âƒ£ ExtracciÃ³n de Palabras Clave con TF-IDF
tokenizer = RegexTokenizer(inputCol="title", outputCol="words", pattern="\\W")
df_tokenized = tokenizer.transform(df_cleaned)

remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
df_filtered = remover.transform(df_tokenized)

hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=20)
df_featurized = hashingTF.transform(df_filtered)

idf = IDF(inputCol="raw_features", outputCol="features")
idf_model = idf.fit(df_featurized)
df_tf_idf = idf_model.transform(df_featurized)

# ğŸ“Œ 8ï¸âƒ£ IdentificaciÃ³n de Entidades (Organizaciones, Personas, Lugares)
nltk.download('punkt')
nltk.download('maxent_ne_chunker')
nltk.download('words')

def extract_entities(text):
    words = word_tokenize(text)
    tagged = pos_tag(words)
    chunked = ne_chunk(tagged)
    
    entities = []
    for chunk in chunked:
        if isinstance(chunk, Tree):
            entity_name = " ".join(c[0] for c in chunk)
            entity_type = chunk.label()
            entities.append(f"{entity_name} ({entity_type})")
    return ", ".join(entities)

extract_entities_udf = udf(extract_entities, StringType())
df_entities = df_cleaned.withColumn("entidades", extract_entities_udf(col("title")))

# ğŸ“Œ 9ï¸âƒ£ ClasificaciÃ³n de ArtÃ­culos por Tema
TOPICS = {
    "Ciencia": ["NASA", "SpaceX", "ciencia", "investigaciÃ³n"],
    "EconomÃ­a": ["mercado", "acciones", "inversiÃ³n"],
    "ExploraciÃ³n": ["Marte", "lanzamiento", "misiÃ³n", "astronauta"]
}

def classify_article(title):
    for topic, keywords in TOPICS.items():
        if any(keyword.lower() in title.lower() for keyword in keywords):
            return topic
    return "Otro"

classify_udf = udf(classify_article, StringType())
df_classified = df_cleaned.withColumn("tema", classify_udf(col("title")))

# ğŸ“Œ ğŸ”Ÿ AnÃ¡lisis de Tendencias
df_tendencias = df_cleaned.groupBy(date_format("fecha_publicacion", "yyyy-MM-dd").alias("fecha")) \
                          .agg(count("id").alias("total_articulos")) \
                          .orderBy("fecha")

df_fuentes_activas = df_cleaned.groupBy("fuente") \
                               .agg(count("id").alias("cantidad_articulos")) \
                               .orderBy(col("cantidad_articulos").desc())

# ğŸ“Œ 1ï¸âƒ£1ï¸âƒ£ Guardar Datos Limpios en Cloud Storage
df_classified.write.mode("overwrite").parquet(PROCESSED_PATH)

# ğŸ“Œ 1ï¸âƒ£2ï¸âƒ£ Cargar Datos a BigQuery
df_classified.write \
    .format("bigquery") \
    .option("table", BIGQUERY_TABLE) \
    .option("temporaryGcsBucket", BUCKET_NAME) \
    .mode("overwrite") \
    .save()

print(f"âœ… Procesamiento finalizado. Datos guardados en {BIGQUERY_TABLE}")

# ğŸ“Œ 1ï¸âƒ£3ï¸âƒ£ Guardar Reportes de Tendencias y Fuentes MÃ¡s Activas
df_tendencias.write.format("bigquery") \
    .option("table", "analitica-contact-center-dev.Entorno_Pruebas_modelo.tendencias_temas") \
    .mode("overwrite").save()

df_fuentes_activas.write.format("bigquery") \
    .option("table", "analitica-contact-center-dev.Entorno_Pruebas_modelo.fuentes_activas") \
    .mode("overwrite").save()

print(f"âœ… Reportes de tendencias y fuentes activas guardados en BigQuery.")

# ğŸ“Œ 1ï¸âƒ£4ï¸âƒ£ Finalizar SesiÃ³n de Spark
spark.stop()

