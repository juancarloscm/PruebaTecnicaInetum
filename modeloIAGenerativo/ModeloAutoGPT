# ğŸš€ Agente AutÃ³nomo en Google Cloud Platform (GCP)

Este proyecto implementa un **agente autÃ³nomo basado en GPT** que se integra con **Google Cloud Platform (GCP)** para realizar las siguientes tareas:
- **Ingesta de datos desde BigQuery**
- **AnÃ¡lisis y enriquecimiento de datos** usando Google Cloud Natural Language y GPT
- **Toma de decisiones automÃ¡ticas** y generaciÃ³n de alertas
- **ActualizaciÃ³n de dashboards en Looker Studio**

El agente estÃ¡ desplegado en **Cloud Run** y se activa mediante eventos de **Pub/Sub**.

---
## ğŸ› ï¸ Arquitectura
1. **Pub/Sub:** Recibe eventos y activa el agente autÃ³nomo.
2. **Cloud Run:** Ejecuta el agente como un servicio serverless.
3. **BigQuery:** Fuente de datos para el anÃ¡lisis.
4. **Google Cloud Natural Language:** Enriquecimiento del texto mediante la identificaciÃ³n de palabras clave y entidades.
5. **GPT (OpenAI):** AnÃ¡lisis avanzado y generaciÃ³n de respuestas.
6. **Looker Studio:** VisualizaciÃ³n en tiempo real de resultados.

### **Flujo de Trabajo:**
1. **Ingesta y Limpieza:** ObtenciÃ³n de datos desde BigQuery.
2. **Enriquecimiento:** ExtracciÃ³n de palabras clave y entidades usando Google Cloud Natural Language.
3. **AnÃ¡lisis de Tendencias:** GPT analiza el contenido para detectar patrones.
4. **Toma de Decisiones:** Se generan alertas o se actualiza el dashboard en Looker Studio.
5. **Respuesta:** El agente responde al evento y finaliza el proceso.

---
## ğŸ“‚ Estructura del Proyecto
```
agente_autonomo/
  â”œâ”€â”€ main.py                 # Script principal del agente
  â”œâ”€â”€ Dockerfile              # Archivo para construir la imagen de Docker
  â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
```

---
## ğŸ“„ CÃ³digo Principal (`main.py`)
```python
from flask import Flask, request, jsonify
import openai
from google.cloud import bigquery, language_v1

# InicializaciÃ³n
app = Flask(__name__)
openai.api_key = "TU_API_KEY"
bq_client = bigquery.Client()
nlp_client = language_v1.LanguageServiceClient()

# FunciÃ³n para consultar BigQuery
def consultar_bigquery(query):
    query_job = bq_client.query(query)
    result = query_job.result()
    return [dict(row.items()) for row in result]

# FunciÃ³n para analizar texto con Google Cloud Natural Language
def extraer_palabras_clave(texto):
    document = language_v1.Document(content=texto, type_=language_v1.Document.Type.PLAIN_TEXT)
    response = nlp_client.analyze_entities(document=document)
    return [entity.name for entity in response.entities if entity.salience > 0.1]

# FunciÃ³n para usar GPT y analizar tendencias
def analizar_tendencias(datos):
    texto = f"Los datos obtenidos son: {datos}. Â¿QuÃ© tendencias importantes puedes identificar?"
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=texto,
        max_tokens=150
    )
    return response.choices[0].text.strip()

# Endpoint principal para Cloud Run
@app.route("/", methods=["POST"])
def recibir_evento():
    mensaje = request.get_json()
    
    # Paso 1: Consultar BigQuery
    query = "SELECT * FROM `analitica-contact-center-dev.dataset.tu_tabla` LIMIT 10"
    datos = consultar_bigquery(query)
    print(f"ğŸ“Š Datos obtenidos de BigQuery: {datos}")

    # Paso 2: Enriquecer los datos con palabras clave
    for dato in datos:
        if "contenido" in dato:
            keywords = extraer_palabras_clave(dato["contenido"])
            dato["palabras_clave"] = keywords
    print(f"ğŸ“ Datos enriquecidos: {datos}")

    # Paso 3: AnÃ¡lisis de tendencias con GPT
    respuesta_gpt = analizar_tendencias(datos)
    print(f"ğŸ“ˆ Respuesta del anÃ¡lisis: {respuesta_gpt}")

    # Paso 4: Decisiones y acciones automÃ¡ticas
    if "alerta" in respuesta_gpt.lower():
        print("âš ï¸ Alerta detectada. Enviando notificaciÃ³n por correo...")
        enviar_alerta("Alerta detectada en el anÃ¡lisis de tendencias.")
    else:
        print("âœ… Todo estÃ¡ en orden.")

    return jsonify({"status": "OK", "message": "Evento procesado correctamente"})

# FunciÃ³n para enviar alerta por correo (simulada)
def enviar_alerta(mensaje):
    print(f"âœ‰ï¸ Enviando alerta: {mensaje}")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080)
```

---
## ğŸ³ Dockerfile
```Dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY . /app
RUN pip install flask google-cloud-bigquery google-cloud-language openai
CMD ["python", "main.py"]
```

---
## ğŸš€ Despliegue en Cloud Run
1. **Construir la imagen de Docker:**
   ```bash
   gcloud builds submit --tag gcr.io/tu-proyecto/agente-autonomo
   ```
2. **Desplegar en Cloud Run:**
   ```bash
   gcloud run deploy agente-autonomo \
       --image gcr.io/tu-proyecto/agente-autonomo \
       --platform managed \
       --region us-central1 \
       --allow-unauthenticated
   ```

---
## ğŸ“¢ IntegraciÃ³n con Pub/Sub
1. **Crear un tema de Pub/Sub:**
   ```bash
   gcloud pubsub topics create agente-eventos
   ```
2. **Configurar una suscripciÃ³n para Cloud Run:**
   ```bash
   gcloud pubsub subscriptions create agente-suscripcion \
       --topic agente-eventos \
       --push-endpoint=https://<YOUR-CLOUD-RUN-URL> \
       --ack-deadline 10
   ```
3. **Publicar un mensaje en Pub/Sub:**
   ```bash
   gcloud pubsub topics publish agente-eventos --message "Evento de prueba"
   ```

---
## ğŸ“Š Monitoreo y Logs
Puedes revisar los logs del agente en Cloud Run:
```bash
gcloud logs read --service=agente-autonomo
```

---
## âœ¨ PrÃ³ximos Pasos
- **Integrar alertas reales por correo o Slack** utilizando **Cloud Functions** para notificar al equipo en tiempo real sobre eventos crÃ­ticos.
- **Desplegar modelos personalizados en Vertex AI**, entrenados con datos histÃ³ricos, para realizar **predicciones mÃ¡s precisas y adaptativas**.
- **Implementar detecciÃ³n automÃ¡tica de anomalÃ­as** usando **BigQuery ML y Vertex AI**, ayudando a identificar patrones inusuales en tiempo real.
- **Optimizar el rendimiento del agente** mediante el uso de **particiones y consultas materializadas en BigQuery**, reduciendo costos y mejorando la velocidad de respuesta.
