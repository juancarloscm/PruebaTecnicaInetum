

#  Pipeline de Analisis de Tendencias en la Industria Espacial 
#  Proyecto: ETL y AnÃ¡lisis de Noticias en GCP

## ğŸ“Œ DescripciÃ³n
Este proyecto implementa un pipeline de **ETL (Extract, Transform, Load)** en **Google Cloud Platform (GCP)** para extraer datos de la API de **Spaceflight News**, transformarlos con **Apache Spark en Dataproc**, y almacenarlos en **Google BigQuery** para anÃ¡lisis y visualizaciÃ³n.

**OPCION 1:**

## âš™ï¸ Tecnologias Utilizadas
- **Google Cloud Composer (Airflow)** - OrquestaciÃ³n del pipeline.
- **Google Cloud Storage (GCS)** - Almacenamiento intermedio de datos.
- **Google BigQuery** - Data Warehouse para anÃ¡lisis y reportes.
- **Google Dataproc (Spark)** - TransformaciÃ³n y procesamiento de datos.
- **Looker Studio** - VisualizaciÃ³n de datos.

## ğŸ“ Estructura del Proyecto
```
â”œâ”€â”€ Dags/
â”‚   â”œâ”€â”€ Dag_blogs_Biquery_Dinamico # etl_almacen_datos_noticias.py-DAG principal en Airflow
â”‚   â”œâ”€â”€ PipelineSpark # PipelineSpark.py # AutomatizaciÃ³n del flujo de trabajo, desde la extracciÃ³n de datos hasta la carga en BigQuery y la generaciÃ³n de insights diarios.
â”‚â”€â”€ Data_Warehouse_Bigquery/
â”‚   â”œâ”€â”€ Fuentes_Noticias_mas_Influyentes.sql  
â”‚   â”œâ”€â”€ Tablas.sql  # sql creacion de tablas 
â”‚   â”œâ”€â”€ Tendencias_Temas_mes.sql  # sql Tendencias
â”‚â”€â”€ sql/
â”‚   â”œâ”€â”€ Relacion_tablas.sql
â”‚   â”œâ”€â”€ dim_fuentes_noticias.sql
â”‚   â”œâ”€â”€ dim_temas.sql
â”‚   â”œâ”€â”€ noticias_procesadas.sql
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ procesamiento_spark.py  # Transformaciones con Spark en Dataproc
â”‚â”€â”€ test_Unitarios/
â”‚   â”œâ”€â”€ test_conectividad_bigquery.py  # Test de integridad del DAG
â”‚   â”œâ”€â”€ test_dag.py  # Test de validaciÃ³n en BigQuery
â”‚â”€â”€ arquitecturas/
â”‚   â”œâ”€â”€ Parte1_Arquitectura_Pipeline
â”‚   â”œâ”€â”€ test_bigquery.py  # Test de validaciÃ³n en BigQuery
â”‚â”€â”€ procesamiento/
â”‚   â”œâ”€â”€ comandos.txt  #
â”‚   â”œâ”€â”€ procesamiento_spark.py  #
â”‚   â”œâ”€â”€ procesamiento_spark_funciones.py  #
â”‚   â”œâ”€â”€ procesamiento_spark_optimizado.py  #
â”‚   â”œâ”€â”€ limpia_y_deduplica.py  # Herramienta: Apache Spark en Google Cloud Dataproc,FunciÃ³n: Limpieza y deduplicaciÃ³n de datos extraÃ­dos (articles, blogs, reports).Almacenamiento: Cloud Storage (cleaned_data.parquet)
â”‚   â”œâ”€â”€ proceso_analysis.py  # Herramienta: Apache Spark en Google Cloud Dataproc,FunciÃ³n: ExtracciÃ³n de palabras clave ClasificaciÃ³n por temas (Launch, Rocket, Space) IdentificaciÃ³n de compaÃ±Ã­as y lugares mencionados Almacenamiento: Cloud Storage (analyzed_data.parquet)
â”‚   â”œâ”€â”€ identfica__topics.py  # Herramienta: Apache Spark en Google Cloud Dataproc FunciÃ³n: AnÃ¡lisis de tendencias por temas Conteo de menciones de compaÃ±Ã­as y lugares Almacenamiento: Google BigQuery (topic_trends, company_mentions, place_mentions)
â”‚â”€â”€ README.md  # DocumentaciÃ³n
```

##  Flujo del Pipeline
1ï¸âƒ£ **ExtracciÃ³n de Datos**: Se extraen noticias desde la API de **Spaceflight News**, manejando paginaciÃ³n y rate limits.
2ï¸âƒ£ **Almacenamiento en GCS**: Los datos se guardan en formato **JSON y Parquet** en Google Cloud Storage.
3ï¸âƒ£ **Procesamiento en Dataproc (Spark)**: Limpieza, deduplicaciÃ³n y anÃ¡lisis de contenido y tendencias.
4ï¸âƒ£ **Carga en BigQuery**: Se insertan datos normalizados en un modelo dimensional.
5ï¸âƒ£ **AnÃ¡lisis SQL**: Se ejecutan consultas optimizadas para tendencias y reportes.
6ï¸âƒ£ **VisualizaciÃ³n en Looker Studio**: Se crean dashboards para anÃ¡lisis de datos.

## ğŸ›  ConfiguraciÃ³n y Despliegue
### 1ï¸âƒ£ Subir el DAG a Composer
```sh
gsutil cp dags/etl_almacen_datos_noticias.py gs://us-central1-flujotransacion-9cfbfa36-bucket/dags/
```

### 2ï¸âƒ£ Subir Script de Spark a GCS
```sh
gsutil cp scripts/procesamiento_spark.py gs://us-central1-flujotransacion-9cfbfa36-bucket/scripts/
```

### 3ï¸âƒ£ Reiniciar Airflow para Aplicar Cambios
```sh
gcloud composer environments restart-web-server flujotransacional --location us-central1
```

### 4ï¸âƒ£ Ejecutar el DAG en Airflow
1. Ir a **Composer â†’ Abrir Airflow**  
2. Activar y Ejecutar el DAG **`etl_almacen_datos_noticias`**  
3. Monitorear la ejecuciÃ³n en **BigQuery**  

## ğŸ§ª Tests Unitarios
Ejecutar pruebas en Airflow y BigQuery:
```sh
pytest tests/
```

## ğŸ“Š AnÃ¡lisis SQL
### ğŸ”¹ **Tendencias de temas por mes**
```sql
SELECT FORMAT_DATE('%Y-%m', fecha_publicacion) AS mes, nombre, COUNT(*) AS total
FROM `analitica-contact-center-dev.Entorno_Pruebas_modelo.fact_articulos`
JOIN `analitica-contact-center-dev.Entorno_Pruebas_modelo.dim_temas`
ON fact_articulos.topic_id = dim_temas.topic_id
GROUP BY mes, nombre ORDER BY mes DESC, total DESC;
```

### ğŸ”¹ **Fuentes de noticias mas influyentes**
```sql
SELECT f.nombre AS fuente, COUNT(a.article_id) AS total_articulos,
       SUM(a.visitas) AS total_visitas, SUM(a.compartidos) AS total_compartidos,
       (SUM(a.visitas) + SUM(a.compartidos)) AS impacto_total
FROM `analitica-contact-center-dev.Entorno_Pruebas_modelo.fact_articulos` a
JOIN `analitica-contact-center-dev.Entorno_Pruebas_modelo.dim_fuentes_noticias` f
ON a.source_id = f.source_id
GROUP BY fuente
ORDER BY impacto_total DESC
LIMIT 10;
```

## ğŸ“ˆ VisualizaciÃ³n en Looker Studio
Conectar **BigQuery** con **Looker Studio** para crear un dashboards interactivo y visualizar tendencias en los datos.

## ğŸ“Œ ConclusiÃ³n
âœ” **Pipeline optimizado con particionamiento y clustering en BigQuery**  
âœ” **Procesamiento escalable en Dataproc con Apache Spark**  
âœ” **OrquestaciÃ³n eficiente con Airflow en Cloud Composer**  
âœ” **VisualizaciÃ³n intuitiva en Looker Studio**  



**OPCION 2**

## Mejora del Modelo
## ğŸ“Œ DescripciÃ³n
ğŸ“Œ Pipeline Completo en Google Cloud
Este pipeline extrae noticias espaciales de la API de Spaceflight News, las procesa y las limpia con Apache Beam (Dataflow), las almacena en BigQuery y las visualiza con Looker Studio.

## âš™ï¸ Tecnologias Utilizadas

- ** 1ï¸âƒ£ Cloud Functions â†’ Ingesta de datos desde la API y publicaciÃ³n en Pub/Sub (100% serverless).
- ** 2ï¸âƒ£ Pub/Sub â†’ Sistema de mensajerÃ­a para manejar datos en tiempo real y desacoplar procesos.
- ** 3ï¸âƒ£ Dataflow (Apache Beam) â†’ Procesa y enriquece los datos (palabras clave, clasificaciÃ³n) antes de enviarlos a BigQuery.
- ** 4ï¸âƒ£ BigQuery â†’ Almacena y analiza grandes volÃºmenes de datos, con particionamiento y clustering para consultas rÃ¡pidas.
- ** 5ï¸âƒ£ Cloud Composer (Airflow) â†’ Orquesta el pipeline completo, programa tareas y monitorea fallos.
- ** 6ï¸âƒ£ Google Cloud Natural Language API â†’ AnÃ¡lisis de texto para extraer entidades y temas principales.
- ** 7ï¸âƒ£ Looker Studio â†’ Dashboards dinÃ¡micos para visualizar tendencias y patrones clave.

Arquitectura PIPELINE 

https://lucid.app/documents/embedded/230b2762-6f66-4fe1-8dac-260179ab6aaf

Inteligencia Artificial utilizada
Modelo Ia-ops
Ver PDF


1ï¸âƒ£ Ingesta de Datos con Cloud Functions
ğŸ“Œ Objetivo:

Extraer noticias de Spaceflight News API.
Enviar los datos a Pub/Sub para su procesamiento.
ğŸ“Œ CÃ³digo en Python para Cloud Functions:

import requests
import json
import time
from google.cloud import pubsub_v1
from datetime import datetime

# ConfiguraciÃ³n de la API y Pub/Sub
BASE_URL = "https://api.spaceflightnewsapi.net/v4"
ENDPOINTS = ["/articles", "/blogs", "/reports", "/info"]
PROJECT_ID = "mi-proyecto"
TOPIC_NAME = "ingesta-noticias"

# Sistema de Logs detallado
def log_event(event):
    """Logea eventos del sistema."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {event}")

def obtener_datos(endpoint):
    """Obtiene todos los datos de un endpoint con paginaciÃ³n y manejo de rate limits."""
    url = f"{BASE_URL}{endpoint}"
    datos = []
    seen_ids = set()  # Para deduplicar artÃ­culos

    while url:
        response = requests.get(url)
        
        # Manejo de rate limit (espera 5 segundos y reintenta)
        if response.status_code == 429:
            log_event("âš ï¸ Rate limit alcanzado. Esperando 5 segundos...")
            time.sleep(5)
            continue
        
        # VerificaciÃ³n de respuesta correcta
        if response.status_code == 200:
            json_data = response.json()
            for item in json_data["results"]:
                # DeduplicaciÃ³n basada en el ID del artÃ­culo
                if item["id"] not in seen_ids:
                    datos.append(item)
                    seen_ids.add(item["id"])
            
            # Obtener la URL de la siguiente pÃ¡gina
            url = json_data.get("next")
            log_event(f"âœ… PÃ¡gina procesada para {endpoint}.")
        else:
            log_event(f"âš ï¸ Error al obtener datos de {endpoint}: {response.status_code}")
            break
    
    return datos

def publicar_en_pubsub(datos, endpoint):
    """Publica los datos en Pub/Sub."""
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path(PROJECT_ID, TOPIC_NAME)
    
    for item in datos:
        mensaje = {
            "endpoint": endpoint,
            "data": item
        }
        publisher.publish(topic_path, data=json.dumps(mensaje).encode("utf-8"))
    
    log_event(f"âœ… {len(datos)} registros del endpoint {endpoint} publicados en Pub/Sub.")

def main(request):
    """FunciÃ³n principal de Cloud Function."""
    log_event("ğŸš€ Inicio de la ingesta de datos.")
    
    for endpoint in ENDPOINTS:
        datos = obtener_datos(endpoint)
        if datos:
            publicar_en_pubsub(datos, endpoint)
        else:
            log_event(f"âš ï¸ No se encontraron datos para {endpoint}.")
    
    log_event("ğŸ¯ Ingesta completada para todos los endpoints.")
    return "âœ… Ingesta finalizada."

ğŸ”¹ Mejoras Implementadas
1ï¸âƒ£ Sistema de PaginaciÃ³n Eficiente: La funciÃ³n obtener_datos gestiona la paginaciÃ³n automÃ¡tica con el campo next de la API.
2ï¸âƒ£ Manejo de Rate Limits: Si se recibe el cÃ³digo 429 (rate limit), el sistema espera 5 segundos y vuelve a intentar la solicitud.
3ï¸âƒ£ DeduplicaciÃ³n de ArtÃ­culos: Se utiliza un conjunto (set) de IDs para evitar duplicados al recopilar datos.
4ï¸âƒ£ Sistema de Logs: Cada acciÃ³n importante se registra con un timestamp para facilitar la auditorÃ­a y depuraciÃ³n.
5ï¸âƒ£ CÃ³digo preparado para Tests Unitarios: Las funciones son modulares y reutilizables, lo que facilita la creaciÃ³n de tests.

ğŸ“Œ Â¿CÃ³mo serÃ­a un Test Unitario para esta funciÃ³n?
test unitario para la funciÃ³n obtener_datos utilizando unittest.

ğŸ“Œ CÃ³digo de Test Unitario :
import unittest
from unittest.mock import patch
import requests

class TestObtenerDatos(unittest.TestCase):

    @patch('requests.get')
    def test_obtener_datos_paginacion(self, mock_get):
        # Simula la respuesta de la API con paginaciÃ³n
        mock_get.side_effect = [
            MockResponse({"results": [{"id": 1, "title": "ArtÃ­culo 1"}], "next": "next_page_url"}, 200),
            MockResponse({"results": [{"id": 2, "title": "ArtÃ­culo 2"}], "next": None}, 200)
        ]
        
        datos = obtener_datos("/articles")
        self.assertEqual(len(datos), 2)
        self.assertEqual(datos[0]["title"], "ArtÃ­culo 1")

class MockResponse:
    def __init__(self, json_data, status_code):
        self.json_data = json_data
        self.status_code = status_code

    def json(self):
        return self.json_data

if __name__ == '__main__':
    unittest.main()
    


ğŸ“Œ Paso 1: Crear las Tablas en BigQuery
Antes de procesar los datos, vamos a crear tablas separadas en BigQuery para almacenar cada tipo de datos (articles, blogs, reports, info).

ğŸ“Œ Tablas en BigQuery:

-- Tabla para artÃ­culos
CREATE TABLE dataset_noticias.articles (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP,
    updated_at TIMESTAMP,
    featured BOOLEAN,
    launches ARRAY<STRING>,
    events ARRAY<STRING>
);

-- Tabla para blogs
CREATE TABLE dataset_noticias.blogs (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP
);

-- Tabla para reports
CREATE TABLE dataset_noticias.reports (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    news_site STRING,
    published_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Tabla para metadata (info)
CREATE TABLE dataset_noticias.info (
    version STRING,
    about STRING
);


ğŸ”¹ Paso 1: DiseÃ±o de Tablas Mejorado
las tablas para soportar anÃ¡lisis avanzado de contenido y tendencias de noticias.

1ï¸âƒ£ Tabla articles optimizada
Incluimos nuevas columnas para almacenar:

Palabras clave (keywords).
Entidades (compaÃ±Ã­as, personas, lugares) extraÃ­das del contenido.
ClasificaciÃ³n por tema (science, politics, technology, etc.).
Particionamiento por fecha (published_at) para mejorar las consultas histÃ³ricas.

ğŸ“Œ Esquema de la Tabla articles:
CREATE TABLE dataset_noticias.articles (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP,
    updated_at TIMESTAMP,
    featured BOOLEAN,
    launches ARRAY<STRING>,
    events ARRAY<STRING>,
    keywords ARRAY<STRING>,                  -- Palabras clave
    entities ARRAY<STRING>,                  -- Entidades identificadas
    topic STRING                             -- ClasificaciÃ³n por tema
)
PARTITION BY DATE(published_at);             -- Particionamiento por fecha

2ï¸âƒ£ Tabla blogs optimizada
AÃ±adimos columnas para:

Palabras clave y entidades.
ClasificaciÃ³n por tema.
Particionamiento por fecha (published_at).
ğŸ“Œ Esquema de la Tabla blogs:
CREATE TABLE dataset_noticias.blogs (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP,
    keywords ARRAY<STRING>,                  -- Palabras clave
    entities ARRAY<STRING>,                  -- Entidades identificadas
    topic STRING                             -- ClasificaciÃ³n por tema
)
PARTITION BY DATE(published_at);

ğŸ”¹ Paso 2: ImplementaciÃ³n de AnÃ¡lisis de Contenido
ğŸ“Œ Objetivo:
Extraer palabras clave, entidades y clasificaciones para enriquecer los datos almacenados.

Utilizaremos Google Cloud Natural Language API para el anÃ¡lisis de texto.

ğŸ“Œ CÃ³digo para AnÃ¡lisis de Contenido (Python):

from google.cloud import language_v1

def analizar_contenido(texto):
    """Extrae palabras clave y entidades del texto usando Natural Language API."""
    client = language_v1.LanguageServiceClient()

    document = language_v1.Document(content=texto, type_=language_v1.Document.Type.PLAIN_TEXT)
    response = client.analyze_entities(document=document)
    
    keywords = [entity.name for entity in response.entities]
    entities = [entity.type_.name for entity in response.entities]

    return keywords, entities

ğŸ“Œ integrar este anÃ¡lisis en el pipeline de Dataflow:

Llama a analizar_contenido para cada summary en los artÃ­culos o blogs.
Almacena las palabras clave y entidades en las columnas correspondientes.

ğŸ”¹ Paso 3: AnÃ¡lisis de Tendencias en BigQuery
ğŸ“Œ Objetivo:
Analizar temas mÃ¡s populares por tiempo y fuentes de noticias mÃ¡s activas.

1ï¸âƒ£ Tendencias por Tema y Tiempo

SELECT topic, DATE(published_at) as fecha, COUNT(*) as total
FROM dataset_noticias.articles
GROUP BY topic, fecha
ORDER BY fecha DESC, total DESC;
2ï¸âƒ£ Fuentes de Noticias MÃ¡s Activas

SELECT news_site, COUNT(*) as total_articulos
FROM dataset_noticias.articles
GROUP BY news_site
ORDER BY total_articulos DESC
LIMIT 10;

ğŸ”¹ Paso 4: OptimizaciÃ³n de Consultas con Particionamiento y Caching
1ï¸âƒ£ Particionamiento
Ya hemos implementado particionamiento por fecha en las tablas. Esto asegura que las consultas histÃ³ricas se ejecuten sobre menos datos, reduciendo costos y tiempo de respuesta.

2ï¸âƒ£ Caching de Resultados Frecuentes
Activa el resultado en cachÃ© en BigQuery para consultas que se ejecutan frecuentemente:

SELECT news_site, COUNT(*) as total_articulos
FROM dataset_noticias.articles
GROUP BY news_site
OPTIONS (allow_large_results=true)


ğŸ“Œ Objetivo:
Enriquecer el contenido de artÃ­culos, blogs y reportes con palabras clave, entidades y clasificaciÃ³n por tema.
Almacenar estos datos enriquecidos en las tablas de BigQuery optimizadas.
Utilizaremos Google Cloud Natural Language API para analizar el contenido y agregar las columnas keywords, entities y topic.

ğŸ“Œ Paso 1: ActualizaciÃ³n del Pipeline de Dataflow
CÃ³digo Completo para el Pipeline con AnÃ¡lisis de Contenido:

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import json
from google.cloud import language_v1

class FiltrarPorEndpoint(beam.DoFn):
    """Filtra los datos por endpoint y realiza anÃ¡lisis de contenido."""
    
    def process(self, element):
        mensaje = json.loads(element.decode('utf-8'))
        endpoint = mensaje["endpoint"]
        data = mensaje["data"]
        
        # Realiza anÃ¡lisis de contenido solo para artÃ­culos, blogs y reports
        if endpoint in ["/articles", "/blogs", "/reports"]:
            data["keywords"], data["entities"] = analizar_contenido(data.get("summary", ""))
            data["topic"] = clasificar_tema(data.get("summary", ""))
        
        yield beam.pvalue.TaggedOutput(endpoint.strip("/"), data)

def analizar_contenido(texto):
    """Extrae palabras clave y entidades usando Google Cloud Natural Language API."""
    client = language_v1.LanguageServiceClient()
    document = language_v1.Document(content=texto, type_=language_v1.Document.Type.PLAIN_TEXT)
    
    response = client.analyze_entities(document=document)
    keywords = [entity.name for entity in response.entities]
    entities = [entity.type_.name for entity in response.entities]
    
    return keywords, entities

def clasificar_tema(texto):
    """Clasifica el texto en temas simples (science, politics, technology, etc.)."""
    if "space" in texto.lower() or "nasa" in texto.lower():
        return "science"
    elif "technology" in texto.lower() or "AI" in texto.lower():
        return "technology"
    elif "government" in texto.lower() or "election" in texto.lower():
        return "politics"
    return "general"

def ejecutar_pipeline():
    """Pipeline principal para procesar y almacenar datos en BigQuery."""
    options = PipelineOptions(streaming=True, project="mi-proyecto", region="us-central1")

    with beam.Pipeline(options=options) as p:
        resultados = (p
                     | "Leer desde Pub/Sub" >> beam.io.ReadFromPubSub(topic="projects/mi-proyecto/topics/ingesta-noticias")
                     | "Filtrar y Analizar" >> beam.ParDo(FiltrarPorEndpoint()).with_outputs(
                         "articles", "blogs", "reports", "info")
                     )
        
        # Escribir datos enriquecidos en BigQuery para cada endpoint
        (resultados.articles
         | "Escribir en BigQuery - Articles" >> beam.io.WriteToBigQuery(
                "mi-proyecto.dataset_noticias.articles",
                schema="id:STRING, title:STRING, url:STRING, image_url:STRING, news_site:STRING, "
                       "summary:STRING, published_at:TIMESTAMP, updated_at:TIMESTAMP, "
                       "featured:BOOLEAN, launches:ARRAY<STRING>, events:ARRAY<STRING>, "
                       "keywords:ARRAY<STRING>, entities:ARRAY<STRING>, topic:STRING",
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
         )

        (resultados.blogs
         | "Escribir en BigQuery - Blogs" >> beam.io.WriteToBigQuery(
                "mi-proyecto.dataset_noticias.blogs",
                schema="id:STRING, title:STRING, url:STRING, image_url:STRING, news_site:STRING, "
                       "summary:STRING, published_at:TIMESTAMP, keywords:ARRAY<STRING>, "
                       "entities:ARRAY<STRING>, topic:STRING",
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
         )

        (resultados.reports
         | "Escribir en BigQuery - Reports" >> beam.io.WriteToBigQuery(
                "mi-proyecto.dataset_noticias.reports",
                schema="id:STRING, title:STRING, url:STRING, news_site:STRING, "
                       "published_at:TIMESTAMP, updated_at:TIMESTAMP, "
                       "keywords:ARRAY<STRING>, entities:ARRAY<STRING>, topic:STRING",
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
         )

if __name__ == "__main__":
    ejecutar_pipeline()

ğŸ“Œ ExplicaciÃ³n del CÃ³digo
1ï¸âƒ£ Filtrado y Enriquecimiento de Datos:
La clase FiltrarPorEndpoint analiza el contenido de cada artÃ­culo, blog y reporte para extraer palabras clave, entidades y clasificar el tema usando Google Cloud Natural Language API.

2ï¸âƒ£ ClasificaciÃ³n de Temas:
La funciÃ³n clasificar_tema identifica el tema principal basÃ¡ndose en palabras clave simples (science, technology, etc.).

3ï¸âƒ£ Almacenamiento en BigQuery:
Los datos enriquecidos se almacenan en tablas separadas (articles, blogs, reports), incluyendo las nuevas columnas (keywords, entities, topic).

ğŸ“Œ Paso 2: Desplegar el Pipeline
1ï¸âƒ£ Guardar el cÃ³digo como dataflow_pipeline.py.
2ï¸âƒ£ Ejecutar el pipeline en Dataflow:

python dataflow_pipeline.py
ğŸ”¹ Paso 3: Verificar y Visualizar los Datos
1ï¸âƒ£ Consulta los datos enriquecidos en BigQuery:

SELECT title, keywords, entities, topic, published_at
FROM dataset_noticias.articles
WHERE topic = 'science'
ORDER BY published_at DESC;
2ï¸âƒ£ Crear un Dashboard en Looker Studio:

Visualiza tendencias de temas y fuentes mÃ¡s activas.
Muestra palabras clave mÃ¡s comunes por tema.



ğŸ”¹ OrquestaciÃ³n con Cloud Composer
ğŸ“Œ Cloud Composer es una versiÃ³n gestionada de Apache Airflow en Google Cloud. Nos permite:

Automatizar el pipeline completo, desde la ingesta hasta la carga de datos en BigQuery.
Gestionar dependencias entre tareas (ej: solo procesar datos si la ingesta fue exitosa).
Programar tareas para que se ejecuten diariamente o en intervalos especÃ­ficos.
Monitorear fallos y reintentos automÃ¡ticos.


[Cloud Functions (Ingesta)] ---> [Dataflow (Procesamiento)] ---> [BigQuery (Almacenamiento)]
          |                             |                                |
          v                             v                                v
    [Cloud Composer (Airflow) Orquesta y Monitorea el flujo completo]



ğŸ”¹ Paso 1: Crear el DAG de Airflow
Un DAG (Directed Acyclic Graph) en Airflow define el flujo de trabajo y las dependencias entre tareas.

ğŸ“Œ CÃ³digo para el DAG (pipeline_dag.py):

from airflow import DAG
from airflow.providers.google.cloud.operators.functions import CloudFunctionInvokeFunctionOperator
from airflow.providers.google.cloud.operators.dataflow import DataflowCreatePythonJobOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
from datetime import datetime, timedelta

# DefiniciÃ³n del DAG
  default_args = {
    'owner': 'user',
    'depends_on_past': False,
    'email': ['juancarloscm@yahoo.com'],  # Correo al que se enviarÃ¡n las alertas
    'email_on_failure': True,             # Enviar correo si falla la tarea
    'email_on_retry': False,              # No enviar correo en reintentos
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


with DAG(
    'pipeline_spaceflight_news',
    default_args=default_args,
    description='Pipeline orquestado para procesar datos de Spaceflight News',
    schedule_interval='0 6 * * *',  # Corre todos los dÃ­as a las 6 AM
    start_date=datetime(2024, 2, 1),
    catchup=False,
) as dag:

    # Tarea 1: Invocar la Cloud Function para la ingesta
    ingesta_task = CloudFunctionInvokeFunctionOperator(
        task_id='invocar_ingesta',
        project_id='mi-proyecto',
        location='us-central1',
        input_data={},
        function_name='ingesta_todos_endpoints'
    )

    # Tarea 2: Ejecutar el job de Dataflow para el procesamiento
    dataflow_task = DataflowCreatePythonJobOperator(
        task_id='procesar_datos_dataflow',
        py_file="gs://mi-bucket/dataflow_pipeline.py",
        job_name='dataflow-pipeline-spaceflight',
        options={
            'project': 'mi-proyecto',
            'region': 'us-central1',
            'streaming': True
        }
    )

    # Tarea 3: VerificaciÃ³n de datos en BigQuery
    verificar_datos_task = BigQueryExecuteQueryOperator(
        task_id='verificar_datos',
        sql='SELECT COUNT(*) FROM dataset_noticias.articles WHERE DATE(published_at) = CURRENT_DATE()',
        use_legacy_sql=False
    )

   generar_reporte_task = BigQueryExecuteQueryOperator(
    task_id='generar_reporte_tendencias',
    sql="""
        CREATE OR REPLACE TABLE dataset_noticias.reporte_tendencias AS
        SELECT 
            topic, 
            COUNT(*) AS total_articulos,
            DATE(published_at) AS fecha
        FROM dataset_noticias.articles
        WHERE DATE(published_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
        GROUP BY topic, fecha
        ORDER BY fecha DESC, total_articulos DESC;
    """,
    use_legacy_sql=False
)

    # Definir el flujo de ejecuciÃ³n
    ingesta_task >> dataflow_task >> verificar_datos_task >> generar_reporte_task

2ï¸âƒ£ se sube el archivo al bucket de Cloud Composer:
gsutil cp pipeline_dag.py gs://BUCKET_DE_COMPOSER/dags/


ğŸ”¹ ExplicaciÃ³n del CÃ³digo
1ï¸âƒ£ DefiniciÃ³n del DAG:

schedule_interval='0 6 * * *' programa la ejecuciÃ³n todos los dÃ­as a las 6 AM.
default_args configura parÃ¡metros comunes para todas las tareas (reintentos, retraso entre reintentos, etc.).
2ï¸âƒ£ CloudFunctionInvokeFunctionOperator: Invoca la Cloud Function para la ingesta de datos.

3ï¸âƒ£ DataflowCreatePythonJobOperator: Ejecuta el script de Dataflow (dataflow_pipeline.py) que procesa y enriquece los datos.

4ï¸âƒ£ BigQueryExecuteQueryOperator: Ejecuta una consulta en BigQuery para verificar que los datos fueron procesados correctamente.

5ï¸âƒ£ Flujo de EjecuciÃ³n (>>): Define las dependencias entre tareas. En este caso:

Primero se ejecuta la ingesta de datos,
Luego se ejecuta el procesamiento en Dataflow,
Finalmente se verifica la existencia de datos en BigQuery.







## IDE de Entendimiento del API
http://190.26.178.21/IDETestGCP/menu.php





