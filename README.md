#  Pipeline de Analisis de Tendencias en la Industria Espacial 
#  Proyecto: ETL y AnÃ¡lisis de Noticias en GCP

## ğŸ“Œ DescripciÃ³n
Este proyecto implementa un pipeline de **ETL (Extract, Transform, Load)** en **Google Cloud Platform (GCP)** para extraer datos de la API de **Spaceflight News**, transformarlos con **Apache Spark en Dataproc**, y almacenarlos en **Google BigQuery** para anÃ¡lisis y visualizaciÃ³n.

**OPCION 1:**

## âš™ï¸ Herramientas Utilizadas
- **Google Cloud Composer (Airflow)** - OrquestaciÃ³n del pipeline.
- **Google Cloud Storage (GCS)** - Almacenamiento intermedio de datos.
- **Google BigQuery** - Data Warehouse para anÃ¡lisis y reportes.
- **Google Dataproc (Spark)** - TransformaciÃ³n y procesamiento de datos.
- **Looker Studio** - VisualizaciÃ³n de datos.

## ğŸ“ Estructura del Proyecto
```
â”œâ”€â”€ Dags/
â”‚   â”œâ”€â”€ Dag_blogs_Biquery_Dinamico # etl_almacen_datos_noticias.py-DAG principal en Airflow
â”‚   â”œâ”€â”€ PipelineSpark # PipelineSpark.py # AutomatizaciÃ³n del flujo de trabajo, desde la extracciÃ³n de datos hasta la carga en BigQuery y la generaciÃ³n de insights diarios.
â”‚â”€â”€ Data_Warehouse_Bigquery/
â”‚   â”œâ”€â”€ Fuentes_Noticias_mas_Influyentes.sql  
â”‚   â”œâ”€â”€ Tablas.sql  # sql creacion de tablas 
â”‚   â”œâ”€â”€ Tendencias_Temas_mes.sql  # sql Tendencias
â”‚â”€â”€ sql/
â”‚   â”œâ”€â”€ Relacion_tablas.sql
â”‚   â”œâ”€â”€ dim_fuentes_noticias.sql
â”‚   â”œâ”€â”€ dim_temas.sql
â”‚   â”œâ”€â”€ noticias_procesadas.sql
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ procesamiento_spark.py  # Transformaciones con Spark en Dataproc
â”‚â”€â”€ test_Unitarios/
â”‚   â”œâ”€â”€ test_conectividad_bigquery.py  # Test de integridad del DAG
â”‚   â”œâ”€â”€ test_dag.py  # Test de validaciÃ³n en BigQuery
â”‚â”€â”€ arquitecturas/
â”‚   â”œâ”€â”€ Parte1_Arquitectura_Pipeline
â”‚   â”œâ”€â”€ test_bigquery.py  # Test de validaciÃ³n en BigQuery
â”‚â”€â”€ procesamiento/
â”‚   â”œâ”€â”€ comandos.txt  #
â”‚   â”œâ”€â”€ procesamiento_spark.py  #
â”‚   â”œâ”€â”€ procesamiento_spark_funciones.py  #
â”‚   â”œâ”€â”€ procesamiento_spark_optimizado.py  #
â”‚   â”œâ”€â”€ limpia_y_deduplica.py  # Herramienta: Apache Spark en Google Cloud Dataproc,FunciÃ³n: Limpieza y deduplicaciÃ³n de datos extraÃ­dos (articles, blogs, reports).Almacenamiento: Cloud Storage (cleaned_data.parquet)
â”‚   â”œâ”€â”€ proceso_analysis.py  # Herramienta: Apache Spark en Google Cloud Dataproc,FunciÃ³n: ExtracciÃ³n de palabras clave ClasificaciÃ³n por temas (Launch, Rocket, Space) IdentificaciÃ³n de compaÃ±Ã­as y lugares mencionados Almacenamiento: Cloud Storage (analyzed_data.parquet)
â”‚   â”œâ”€â”€ identfica__topics.py  # Herramienta: Apache Spark en Google Cloud Dataproc FunciÃ³n: AnÃ¡lisis de tendencias por temas Conteo de menciones de compaÃ±Ã­as y lugares Almacenamiento: Google BigQuery (topic_trends, company_mentions, place_mentions)
â”‚â”€â”€ README.md  # DocumentaciÃ³n
```

##  Flujo del Pipeline
- ** 1ï¸âƒ£ **ExtracciÃ³n de Datos**: Se extraen noticias desde la API de **Spaceflight News**, manejando paginaciÃ³n y rate limits.
- ** 2ï¸âƒ£ **Almacenamiento en GCS**: Los datos se guardan en formato **JSON y Parquet** en Google Cloud Storage.
- ** 3ï¸âƒ£ **Procesamiento en Dataproc (Spark)**: Limpieza, deduplicaciÃ³n y anÃ¡lisis de contenido y tendencias.
- ** 4ï¸âƒ£ **Carga en BigQuery**: Se insertan datos normalizados en un modelo dimensional.
- ** 5ï¸âƒ£ **AnÃ¡lisis SQL**: Se ejecutan consultas optimizadas para tendencias y reportes.
- ** 6ï¸âƒ£ **VisualizaciÃ³n en Looker Studio**: Se crean dashboards para anÃ¡lisis de datos.

## ğŸ›  ConfiguraciÃ³n y Despliegue
- ** ### 1ï¸âƒ£ Subir el DAG a Composer
```sh
gsutil cp dags/etl_almacen_datos_noticias.py gs://us-central1-flujotransacion-9cfbfa36-bucket/dags/
```

- ** ### 2ï¸âƒ£ Subir Script de Spark a GCS
```sh
gsutil cp scripts/procesamiento_spark.py gs://us-central1-flujotransacion-9cfbfa36-bucket/scripts/
```

### 3ï¸âƒ£ Reiniciar Airflow para Aplicar Cambios
```sh
gcloud composer environments restart-web-server flujotransacional --location us-central1
```

### 4ï¸âƒ£ Ejecutar el DAG en Airflow
1. Ir a **Composer â†’ Abrir Airflow**  
2. Activar y Ejecutar el DAG **`etl_almacen_datos_noticias`**  
3. Monitorear la ejecuciÃ³n en **BigQuery**  

## ğŸ§ª Tests Unitarios
Ejecutar pruebas en Airflow y BigQuery:
```sh
pytest tests/
```

## ğŸ“Š AnÃ¡lisis SQL
### ğŸ”¹ **Tendencias de temas por mes** analisis_tendencias.sql 

### ğŸ”¹ **Fuentes de noticias mas influyentes** fuentes_mas_influyentes.sql

## ğŸ“ˆ VisualizaciÃ³n en Looker Studio
Conectar **BigQuery** con **Looker Studio** para crear un dashboards interactivo y visualizar tendencias en los datos.

## ğŸ“Œ ConclusiÃ³n
âœ” **Pipeline optimizado con particionamiento y clustering en BigQuery**  
âœ” **Procesamiento escalable en Dataproc con Apache Spark**  
âœ” **OrquestaciÃ³n eficiente con Airflow en Cloud Composer**  
âœ” **VisualizaciÃ³n intuitiva en Looker Studio**  



**OPCION 2**

## Propuesta de Mejora del Modelo con 100% serveless
## ğŸ“Œ DescripciÃ³n
ğŸ“Œ Pipeline Completo en Google Cloud
Este pipeline extrae noticias espaciales de la API de Spaceflight News, las procesa y las limpia con Apache Beam (Dataflow), las almacena en BigQuery y las visualiza con Looker Studio.

## âš™ï¸ Tecnologias Utilizadas

- ** 1ï¸âƒ£ Cloud Functions â†’ Ingesta de datos desde la API y publicaciÃ³n en Pub/Sub (100% serverless).
- ** 2ï¸âƒ£ Pub/Sub â†’ Sistema de mensajerÃ­a para manejar datos en tiempo real y desacoplar procesos.
- ** 3ï¸âƒ£ Dataflow (Apache Beam) â†’ Procesa y enriquece los datos (palabras clave, clasificaciÃ³n) antes de enviarlos a BigQuery.
- ** 4ï¸âƒ£ BigQuery â†’ Almacena y analiza grandes volÃºmenes de datos, con particionamiento y clustering para consultas rÃ¡pidas.
- ** 5ï¸âƒ£ Cloud Composer (Airflow) â†’ Orquesta el pipeline completo, programa tareas y monitorea fallos.
- ** 6ï¸âƒ£ Google Cloud Natural Language API â†’ AnÃ¡lisis de texto para extraer entidades y temas principales.
- ** 7ï¸âƒ£ Looker Studio â†’ Dashboards dinÃ¡micos para visualizar tendencias y patrones clave.

Arquitectura PIPELINE 

https://lucid.app/documents/embedded/230b2762-6f66-4fe1-8dac-260179ab6aaf

Inteligencia Artificial utilizada
Modelo Ia-ops
Ver PDF

##  Sistema de Backup y RecuperaciÃ³n
- ** Backup de datos crÃ­ticos (almacenados en Cloud Storage, BigQuery, y metadatos del pipeline).
- ** AutomatizaciÃ³n de backups periÃ³dicos.
- ** RecuperaciÃ³n rÃ¡pida en caso de fallo o pÃ©rdida de datos.

## ğŸ“Š Resumen de la Arquitectura de Backup
- ** Cloud Storage para respaldar datos intermedios y JSON de entrada.
- ** BigQuery Export para respaldar tablas finales.
- ** Cloud Scheduler para ejecutar tareas automÃ¡ticas.
- ** Google Cloud Monitoring para detectar y alertar sobre errores.


 ## ğŸ›  Estrategia de ImplementaciÃ³n
- ** 1. Backup en Google Cloud Storage
- ** ğŸ“¦ Datos a respaldar:
- ** Datos intermedios (cleaned_data.parquet, analyzed_data.parquet).
- ** Archivos JSON de entrada (articles.json, blogs.json).
- ** ğŸ’¡ CÃ³mo implementarlo:
- ** Configura versiones de objetos en tu bucket (Object Versioning).
- ** Automatiza los backups con un script y programa la tarea en Cloud Scheduler.

- ** 2. Backup de BigQuery
- ** ğŸ“¦ Datos a respaldar:
- ** Tablas finales (topic_trends, company_mentions, place_mentions).
- ** ğŸ’¡ CÃ³mo implementarlo:
- ** Exporta las tablas a Cloud Storage en formato Avro o Parquet.

- ** 3. RecuperaciÃ³n de Datos
- ** Datos en Cloud Storage:
- ** Si tienes versiones anteriores, puedes restaurarlas directamente.
- **  gsutil cp gs://buckets-aws-backup/processed_data_20250201 gs://buckets-aws/processed_data

- ** Datos en BigQuery:
- ** Si las tablas se perdieron, puedes volver a importarlas desde el backup en Cloud Storage.
- ** bq load \
- **   --source_format=PARQUET \
- **  "analitica-contact-center-dev:pos_analitica_ANALISIS.topic_trends" \
- **  "gs://buckets-aws-backup/bigquery/topic_trends_20250201.parquet"

- ** 4. MonitorizaciÃ³n y Alertas
- ** Se Configura Google Cloud Monitoring para recibir alertas en caso de:
- ** Fallos en las tareas de backup.
- ** Falta de espacio en Cloud Storage.
- ** Fallos de importaciÃ³n en BigQuery.





## IDE de Entendimiento del API
http://190.26.178.21/IDETestGCP/menu.php





