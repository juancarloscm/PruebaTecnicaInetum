

#  Pipeline de Analisis de Tendencias en la Industria Espacial 
#  Proyecto: ETL y An√°lisis de Noticias en GCP

## üìå Descripci√≥n
Este proyecto implementa un pipeline de **ETL (Extract, Transform, Load)** en **Google Cloud Platform (GCP)** para extraer datos de la API de **Spaceflight News**, transformarlos con **Apache Spark en Dataproc**, y almacenarlos en **Google BigQuery** para an√°lisis y visualizaci√≥n.

## ‚öôÔ∏è Tecnologias Utilizadas
- **Google Cloud Composer (Airflow)** - Orquestaci√≥n del pipeline.
- **Google Cloud Storage (GCS)** - Almacenamiento intermedio de datos.
- **Google BigQuery** - Data Warehouse para an√°lisis y reportes.
- **Google Dataproc (Spark)** - Transformaci√≥n y procesamiento de datos.
- **Looker Studio** - Visualizaci√≥n de datos.

## üìÅ Estructura del Proyecto
```
‚îú‚îÄ‚îÄ Dags/
‚îÇ   ‚îú‚îÄ‚îÄ Dag_blogs_Biquery_Dinamico # etl_almacen_datos_noticias.py-DAG principal en Airflow
‚îÇ‚îÄ‚îÄ Data_Warehouse_Bigquery/
‚îÇ   ‚îú‚îÄ‚îÄ Fuentes_Noticias_mas_Influyentes.sql  
‚îÇ   ‚îú‚îÄ‚îÄ Tablas.sql  # sql creacion de tablas 
‚îÇ   ‚îú‚îÄ‚îÄ Tendencias_Temas_mes.sql  # sql Tendencias
‚îÇ‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ Relacion_tablas.sql
‚îÇ   ‚îú‚îÄ‚îÄ dim_fuentes_noticias.sql
‚îÇ   ‚îú‚îÄ‚îÄ dim_temas.sql
‚îÇ   ‚îú‚îÄ‚îÄ noticias_procesadas.sql
‚îÇ‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ procesamiento_spark.py  # Transformaciones con Spark en Dataproc
‚îÇ‚îÄ‚îÄ test_Unitarios/
‚îÇ   ‚îú‚îÄ‚îÄ test_conectividad_bigquery.py  # Test de integridad del DAG
‚îÇ   ‚îú‚îÄ‚îÄ test_dag.py  # Test de validaci√≥n en BigQuery
‚îÇ‚îÄ‚îÄ arquitecturas/
‚îÇ   ‚îú‚îÄ‚îÄ Parte1_Arquitectura_Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ test_bigquery.py  # Test de validaci√≥n en BigQuery
‚îÇ‚îÄ‚îÄ procesamiento/
‚îÇ   ‚îú‚îÄ‚îÄ comandos.txt  #
‚îÇ   ‚îú‚îÄ‚îÄ procesamiento_spark.py  #
‚îÇ   ‚îú‚îÄ‚îÄ procesamiento_spark_funciones.py  #
‚îÇ   ‚îú‚îÄ‚îÄ procesamiento_spark_optimizado.py  #
‚îÇ‚îÄ‚îÄ README.md  # Documentaci√≥n
```

##  Flujo del Pipeline
1Ô∏è‚É£ **Extracci√≥n de Datos**: Se extraen noticias desde la API de **Spaceflight News**, manejando paginaci√≥n y rate limits.
2Ô∏è‚É£ **Almacenamiento en GCS**: Los datos se guardan en formato **JSON y Parquet** en Google Cloud Storage.
3Ô∏è‚É£ **Procesamiento en Dataproc (Spark)**: Limpieza, deduplicaci√≥n y an√°lisis de contenido y tendencias.
4Ô∏è‚É£ **Carga en BigQuery**: Se insertan datos normalizados en un modelo dimensional.
5Ô∏è‚É£ **An√°lisis SQL**: Se ejecutan consultas optimizadas para tendencias y reportes.
6Ô∏è‚É£ **Visualizaci√≥n en Looker Studio**: Se crean dashboards para an√°lisis de datos.

## üõ† Configuraci√≥n y Despliegue
### 1Ô∏è‚É£ Subir el DAG a Composer
```sh
gsutil cp dags/etl_almacen_datos_noticias.py gs://us-central1-flujotransacion-9cfbfa36-bucket/dags/
```

### 2Ô∏è‚É£ Subir Script de Spark a GCS
```sh
gsutil cp scripts/procesamiento_spark.py gs://us-central1-flujotransacion-9cfbfa36-bucket/scripts/
```

### 3Ô∏è‚É£ Reiniciar Airflow para Aplicar Cambios
```sh
gcloud composer environments restart-web-server flujotransacional --location us-central1
```

### 4Ô∏è‚É£ Ejecutar el DAG en Airflow
1. Ir a **Composer ‚Üí Abrir Airflow**  
2. Activar y Ejecutar el DAG **`etl_almacen_datos_noticias`**  
3. Monitorear la ejecuci√≥n en **BigQuery**  

## üß™ Tests Unitarios
Ejecutar pruebas en Airflow y BigQuery:
```sh
pytest tests/
```

## üìä An√°lisis SQL
### üîπ **Tendencias de temas por mes**
```sql
SELECT FORMAT_DATE('%Y-%m', fecha_publicacion) AS mes, nombre, COUNT(*) AS total
FROM `analitica-contact-center-dev.Entorno_Pruebas_modelo.fact_articulos`
JOIN `analitica-contact-center-dev.Entorno_Pruebas_modelo.dim_temas`
ON fact_articulos.topic_id = dim_temas.topic_id
GROUP BY mes, nombre ORDER BY mes DESC, total DESC;
```

### üîπ **Fuentes de noticias mas influyentes**
```sql
SELECT f.nombre AS fuente, COUNT(a.article_id) AS total_articulos,
       SUM(a.visitas) AS total_visitas, SUM(a.compartidos) AS total_compartidos,
       (SUM(a.visitas) + SUM(a.compartidos)) AS impacto_total
FROM `analitica-contact-center-dev.Entorno_Pruebas_modelo.fact_articulos` a
JOIN `analitica-contact-center-dev.Entorno_Pruebas_modelo.dim_fuentes_noticias` f
ON a.source_id = f.source_id
GROUP BY fuente
ORDER BY impacto_total DESC
LIMIT 10;
```

## üìà Visualizaci√≥n en Looker Studio
Conectar **BigQuery** con **Looker Studio** para crear un dashboards interactivo y visualizar tendencias en los datos.

## üìå Conclusi√≥n
‚úî **Pipeline optimizado con particionamiento y clustering en BigQuery**  
‚úî **Procesamiento escalable en Dataproc con Apache Spark**  
‚úî **Orquestaci√≥n eficiente con Airflow en Cloud Composer**  
‚úî **Visualizaci√≥n intuitiva en Looker Studio**  


## Mejora del Modelo
## üìå Descripci√≥n
üìå Pipeline Completo en Google Cloud
Este pipeline extrae noticias espaciales de la API de Spaceflight News, las procesa y las limpia con Apache Beam (Dataflow), las almacena en BigQuery y las visualiza con Looker Studio.


https://lucid.app/documents/embedded/230b2762-6f66-4fe1-8dac-260179ab6aaf



                      +------------------------------+
                      |   Spaceflight News API       |
                      +--------------+--------------+
                                     |
                                     v
                     +------------------------------+
                     |    Cloud Functions (ETL)     |
                     | - Llama API diariamente      |
                     | - Publica datos en Pub/Sub   |
                     +--------------+--------------+
                                     |
                                     v
                     +------------------------------+
                     |        Pub/Sub (Mensajes)    |
                     | - Cola de procesamiento      |
                     | - Garantiza escalabilidad    |
                     +--------------+--------------+
                                     |
                                     v
  +-------------------------------------------+------------------------------------------+
  |                                           |                                          |
  |                                           |                                          |
  v                                           v                                          v
+--------------------+         +----------------------------+        +------------------------------+
|   Cloud Storage   |         |  Dataflow (Apache Beam)    |        |  BigQuery (Data Warehouse)   |
| - Guarda datos    |         | - Filtra y limpia datos    |        | - Almacena datos optimizados |
| - Backup & Logs   |         | - Deduplicaci√≥n           |        | - SQL para an√°lisis r√°pidos  |
+--------------------+         +----------------------------+        +------------------------------+
                                                                      |
                                                                      v
                                                         +--------------------------+
                                                         | Looker Studio / DataViz  |
                                                         | - Dashboards din√°micos   |
                                                         | - An√°lisis en tiempo real|
                                                         +--------------------------+



1Ô∏è‚É£ Ingesta de Datos con Cloud Functions
üìå Objetivo:

Extraer noticias de Spaceflight News API.
Enviar los datos a Pub/Sub para su procesamiento.
üìå C√≥digo en Python para Cloud Functions:

import requests
import json
import time
from google.cloud import pubsub_v1
from datetime import datetime

# Configuraci√≥n de la API y Pub/Sub
BASE_URL = "https://api.spaceflightnewsapi.net/v4"
ENDPOINTS = ["/articles", "/blogs", "/reports", "/info"]
PROJECT_ID = "mi-proyecto"
TOPIC_NAME = "ingesta-noticias"

# Sistema de Logs detallado
def log_event(event):
    """Logea eventos del sistema."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {event}")

def obtener_datos(endpoint):
    """Obtiene todos los datos de un endpoint con paginaci√≥n y manejo de rate limits."""
    url = f"{BASE_URL}{endpoint}"
    datos = []
    seen_ids = set()  # Para deduplicar art√≠culos

    while url:
        response = requests.get(url)
        
        # Manejo de rate limit (espera 5 segundos y reintenta)
        if response.status_code == 429:
            log_event("‚ö†Ô∏è Rate limit alcanzado. Esperando 5 segundos...")
            time.sleep(5)
            continue
        
        # Verificaci√≥n de respuesta correcta
        if response.status_code == 200:
            json_data = response.json()
            for item in json_data["results"]:
                # Deduplicaci√≥n basada en el ID del art√≠culo
                if item["id"] not in seen_ids:
                    datos.append(item)
                    seen_ids.add(item["id"])
            
            # Obtener la URL de la siguiente p√°gina
            url = json_data.get("next")
            log_event(f"‚úÖ P√°gina procesada para {endpoint}.")
        else:
            log_event(f"‚ö†Ô∏è Error al obtener datos de {endpoint}: {response.status_code}")
            break
    
    return datos

def publicar_en_pubsub(datos, endpoint):
    """Publica los datos en Pub/Sub."""
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path(PROJECT_ID, TOPIC_NAME)
    
    for item in datos:
        mensaje = {
            "endpoint": endpoint,
            "data": item
        }
        publisher.publish(topic_path, data=json.dumps(mensaje).encode("utf-8"))
    
    log_event(f"‚úÖ {len(datos)} registros del endpoint {endpoint} publicados en Pub/Sub.")

def main(request):
    """Funci√≥n principal de Cloud Function."""
    log_event("üöÄ Inicio de la ingesta de datos.")
    
    for endpoint in ENDPOINTS:
        datos = obtener_datos(endpoint)
        if datos:
            publicar_en_pubsub(datos, endpoint)
        else:
            log_event(f"‚ö†Ô∏è No se encontraron datos para {endpoint}.")
    
    log_event("üéØ Ingesta completada para todos los endpoints.")
    return "‚úÖ Ingesta finalizada."

üîπ Mejoras Implementadas
1Ô∏è‚É£ Sistema de Paginaci√≥n Eficiente: La funci√≥n obtener_datos gestiona la paginaci√≥n autom√°tica con el campo next de la API.
2Ô∏è‚É£ Manejo de Rate Limits: Si se recibe el c√≥digo 429 (rate limit), el sistema espera 5 segundos y vuelve a intentar la solicitud.
3Ô∏è‚É£ Deduplicaci√≥n de Art√≠culos: Se utiliza un conjunto (set) de IDs para evitar duplicados al recopilar datos.
4Ô∏è‚É£ Sistema de Logs: Cada acci√≥n importante se registra con un timestamp para facilitar la auditor√≠a y depuraci√≥n.
5Ô∏è‚É£ C√≥digo preparado para Tests Unitarios: Las funciones son modulares y reutilizables, lo que facilita la creaci√≥n de tests.

üìå ¬øC√≥mo ser√≠a un Test Unitario para esta funci√≥n?
test unitario para la funci√≥n obtener_datos utilizando unittest.

üìå C√≥digo de Test Unitario :
import unittest
from unittest.mock import patch
import requests

class TestObtenerDatos(unittest.TestCase):

    @patch('requests.get')
    def test_obtener_datos_paginacion(self, mock_get):
        # Simula la respuesta de la API con paginaci√≥n
        mock_get.side_effect = [
            MockResponse({"results": [{"id": 1, "title": "Art√≠culo 1"}], "next": "next_page_url"}, 200),
            MockResponse({"results": [{"id": 2, "title": "Art√≠culo 2"}], "next": None}, 200)
        ]
        
        datos = obtener_datos("/articles")
        self.assertEqual(len(datos), 2)
        self.assertEqual(datos[0]["title"], "Art√≠culo 1")

class MockResponse:
    def __init__(self, json_data, status_code):
        self.json_data = json_data
        self.status_code = status_code

    def json(self):
        return self.json_data

if __name__ == '__main__':
    unittest.main()
    


üìå Paso 1: Crear las Tablas en BigQuery
Antes de procesar los datos, vamos a crear tablas separadas en BigQuery para almacenar cada tipo de datos (articles, blogs, reports, info).

üìå Tablas en BigQuery:

-- Tabla para art√≠culos
CREATE TABLE dataset_noticias.articles (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP,
    updated_at TIMESTAMP,
    featured BOOLEAN,
    launches ARRAY<STRING>,
    events ARRAY<STRING>
);

-- Tabla para blogs
CREATE TABLE dataset_noticias.blogs (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP
);

-- Tabla para reports
CREATE TABLE dataset_noticias.reports (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    news_site STRING,
    published_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Tabla para metadata (info)
CREATE TABLE dataset_noticias.info (
    version STRING,
    about STRING
);


üîπ Paso 1: Dise√±o de Tablas Mejorado
las tablas para soportar an√°lisis avanzado de contenido y tendencias de noticias.

1Ô∏è‚É£ Tabla articles optimizada
Incluimos nuevas columnas para almacenar:

Palabras clave (keywords).
Entidades (compa√±√≠as, personas, lugares) extra√≠das del contenido.
Clasificaci√≥n por tema (science, politics, technology, etc.).
Particionamiento por fecha (published_at) para mejorar las consultas hist√≥ricas.

üìå Esquema de la Tabla articles:
CREATE TABLE dataset_noticias.articles (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP,
    updated_at TIMESTAMP,
    featured BOOLEAN,
    launches ARRAY<STRING>,
    events ARRAY<STRING>,
    keywords ARRAY<STRING>,                  -- Palabras clave
    entities ARRAY<STRING>,                  -- Entidades identificadas
    topic STRING                             -- Clasificaci√≥n por tema
)
PARTITION BY DATE(published_at);             -- Particionamiento por fecha

2Ô∏è‚É£ Tabla blogs optimizada
A√±adimos columnas para:

Palabras clave y entidades.
Clasificaci√≥n por tema.
Particionamiento por fecha (published_at).
üìå Esquema de la Tabla blogs:
CREATE TABLE dataset_noticias.blogs (
    id STRING PRIMARY KEY,
    title STRING,
    url STRING,
    image_url STRING,
    news_site STRING,
    summary STRING,
    published_at TIMESTAMP,
    keywords ARRAY<STRING>,                  -- Palabras clave
    entities ARRAY<STRING>,                  -- Entidades identificadas
    topic STRING                             -- Clasificaci√≥n por tema
)
PARTITION BY DATE(published_at);

üîπ Paso 2: Implementaci√≥n de An√°lisis de Contenido
üìå Objetivo:
Extraer palabras clave, entidades y clasificaciones para enriquecer los datos almacenados.

Utilizaremos Google Cloud Natural Language API para el an√°lisis de texto.

üìå C√≥digo para An√°lisis de Contenido (Python):

from google.cloud import language_v1

def analizar_contenido(texto):
    """Extrae palabras clave y entidades del texto usando Natural Language API."""
    client = language_v1.LanguageServiceClient()

    document = language_v1.Document(content=texto, type_=language_v1.Document.Type.PLAIN_TEXT)
    response = client.analyze_entities(document=document)
    
    keywords = [entity.name for entity in response.entities]
    entities = [entity.type_.name for entity in response.entities]

    return keywords, entities

üìå integrar este an√°lisis en el pipeline de Dataflow:

Llama a analizar_contenido para cada summary en los art√≠culos o blogs.
Almacena las palabras clave y entidades en las columnas correspondientes.

üîπ Paso 3: An√°lisis de Tendencias en BigQuery
üìå Objetivo:
Analizar temas m√°s populares por tiempo y fuentes de noticias m√°s activas.

1Ô∏è‚É£ Tendencias por Tema y Tiempo

SELECT topic, DATE(published_at) as fecha, COUNT(*) as total
FROM dataset_noticias.articles
GROUP BY topic, fecha
ORDER BY fecha DESC, total DESC;
2Ô∏è‚É£ Fuentes de Noticias M√°s Activas

SELECT news_site, COUNT(*) as total_articulos
FROM dataset_noticias.articles
GROUP BY news_site
ORDER BY total_articulos DESC
LIMIT 10;

üîπ Paso 4: Optimizaci√≥n de Consultas con Particionamiento y Caching
1Ô∏è‚É£ Particionamiento
Ya hemos implementado particionamiento por fecha en las tablas. Esto asegura que las consultas hist√≥ricas se ejecuten sobre menos datos, reduciendo costos y tiempo de respuesta.

2Ô∏è‚É£ Caching de Resultados Frecuentes
Activa el resultado en cach√© en BigQuery para consultas que se ejecutan frecuentemente:

SELECT news_site, COUNT(*) as total_articulos
FROM dataset_noticias.articles
GROUP BY news_site
OPTIONS (allow_large_results=true)


üìå Objetivo:
Enriquecer el contenido de art√≠culos, blogs y reportes con palabras clave, entidades y clasificaci√≥n por tema.
Almacenar estos datos enriquecidos en las tablas de BigQuery optimizadas.
Utilizaremos Google Cloud Natural Language API para analizar el contenido y agregar las columnas keywords, entities y topic.

üìå Paso 1: Actualizaci√≥n del Pipeline de Dataflow
C√≥digo Completo para el Pipeline con An√°lisis de Contenido:

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import json
from google.cloud import language_v1

class FiltrarPorEndpoint(beam.DoFn):
    """Filtra los datos por endpoint y realiza an√°lisis de contenido."""
    
    def process(self, element):
        mensaje = json.loads(element.decode('utf-8'))
        endpoint = mensaje["endpoint"]
        data = mensaje["data"]
        
        # Realiza an√°lisis de contenido solo para art√≠culos, blogs y reports
        if endpoint in ["/articles", "/blogs", "/reports"]:
            data["keywords"], data["entities"] = analizar_contenido(data.get("summary", ""))
            data["topic"] = clasificar_tema(data.get("summary", ""))
        
        yield beam.pvalue.TaggedOutput(endpoint.strip("/"), data)

def analizar_contenido(texto):
    """Extrae palabras clave y entidades usando Google Cloud Natural Language API."""
    client = language_v1.LanguageServiceClient()
    document = language_v1.Document(content=texto, type_=language_v1.Document.Type.PLAIN_TEXT)
    
    response = client.analyze_entities(document=document)
    keywords = [entity.name for entity in response.entities]
    entities = [entity.type_.name for entity in response.entities]
    
    return keywords, entities

def clasificar_tema(texto):
    """Clasifica el texto en temas simples (science, politics, technology, etc.)."""
    if "space" in texto.lower() or "nasa" in texto.lower():
        return "science"
    elif "technology" in texto.lower() or "AI" in texto.lower():
        return "technology"
    elif "government" in texto.lower() or "election" in texto.lower():
        return "politics"
    return "general"

def ejecutar_pipeline():
    """Pipeline principal para procesar y almacenar datos en BigQuery."""
    options = PipelineOptions(streaming=True, project="mi-proyecto", region="us-central1")

    with beam.Pipeline(options=options) as p:
        resultados = (p
                     | "Leer desde Pub/Sub" >> beam.io.ReadFromPubSub(topic="projects/mi-proyecto/topics/ingesta-noticias")
                     | "Filtrar y Analizar" >> beam.ParDo(FiltrarPorEndpoint()).with_outputs(
                         "articles", "blogs", "reports", "info")
                     )
        
        # Escribir datos enriquecidos en BigQuery para cada endpoint
        (resultados.articles
         | "Escribir en BigQuery - Articles" >> beam.io.WriteToBigQuery(
                "mi-proyecto.dataset_noticias.articles",
                schema="id:STRING, title:STRING, url:STRING, image_url:STRING, news_site:STRING, "
                       "summary:STRING, published_at:TIMESTAMP, updated_at:TIMESTAMP, "
                       "featured:BOOLEAN, launches:ARRAY<STRING>, events:ARRAY<STRING>, "
                       "keywords:ARRAY<STRING>, entities:ARRAY<STRING>, topic:STRING",
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
         )

        (resultados.blogs
         | "Escribir en BigQuery - Blogs" >> beam.io.WriteToBigQuery(
                "mi-proyecto.dataset_noticias.blogs",
                schema="id:STRING, title:STRING, url:STRING, image_url:STRING, news_site:STRING, "
                       "summary:STRING, published_at:TIMESTAMP, keywords:ARRAY<STRING>, "
                       "entities:ARRAY<STRING>, topic:STRING",
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
         )

        (resultados.reports
         | "Escribir en BigQuery - Reports" >> beam.io.WriteToBigQuery(
                "mi-proyecto.dataset_noticias.reports",
                schema="id:STRING, title:STRING, url:STRING, news_site:STRING, "
                       "published_at:TIMESTAMP, updated_at:TIMESTAMP, "
                       "keywords:ARRAY<STRING>, entities:ARRAY<STRING>, topic:STRING",
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
         )

if __name__ == "__main__":
    ejecutar_pipeline()

üìå Explicaci√≥n del C√≥digo
1Ô∏è‚É£ Filtrado y Enriquecimiento de Datos:
La clase FiltrarPorEndpoint analiza el contenido de cada art√≠culo, blog y reporte para extraer palabras clave, entidades y clasificar el tema usando Google Cloud Natural Language API.

2Ô∏è‚É£ Clasificaci√≥n de Temas:
La funci√≥n clasificar_tema identifica el tema principal bas√°ndose en palabras clave simples (science, technology, etc.).

3Ô∏è‚É£ Almacenamiento en BigQuery:
Los datos enriquecidos se almacenan en tablas separadas (articles, blogs, reports), incluyendo las nuevas columnas (keywords, entities, topic).

üìå Paso 2: Desplegar el Pipeline
1Ô∏è‚É£ Guardar el c√≥digo como dataflow_pipeline.py.
2Ô∏è‚É£ Ejecutar el pipeline en Dataflow:

python dataflow_pipeline.py
üîπ Paso 3: Verificar y Visualizar los Datos
1Ô∏è‚É£ Consulta los datos enriquecidos en BigQuery:

SELECT title, keywords, entities, topic, published_at
FROM dataset_noticias.articles
WHERE topic = 'science'
ORDER BY published_at DESC;
2Ô∏è‚É£ Crear un Dashboard en Looker Studio:

Visualiza tendencias de temas y fuentes m√°s activas.
Muestra palabras clave m√°s comunes por tema.



üîπ Orquestaci√≥n con Cloud Composer
üìå Cloud Composer es una versi√≥n gestionada de Apache Airflow en Google Cloud. Nos permite:

Automatizar el pipeline completo, desde la ingesta hasta la carga de datos en BigQuery.
Gestionar dependencias entre tareas (ej: solo procesar datos si la ingesta fue exitosa).
Programar tareas para que se ejecuten diariamente o en intervalos espec√≠ficos.
Monitorear fallos y reintentos autom√°ticos.


[Cloud Functions (Ingesta)] ---> [Dataflow (Procesamiento)] ---> [BigQuery (Almacenamiento)]
          |                             |                                |
          v                             v                                v
    [Cloud Composer (Airflow) Orquesta y Monitorea el flujo completo]



üîπ Paso 1: Crear el DAG de Airflow
Un DAG (Directed Acyclic Graph) en Airflow define el flujo de trabajo y las dependencias entre tareas.

üìå C√≥digo para el DAG (pipeline_dag.py):

from airflow import DAG
from airflow.providers.google.cloud.operators.functions import CloudFunctionInvokeFunctionOperator
from airflow.providers.google.cloud.operators.dataflow import DataflowCreatePythonJobOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
from datetime import datetime, timedelta

# Definici√≥n del DAG
  default_args = {
    'owner': 'user',
    'depends_on_past': False,
    'email': ['juancarloscm@yahoo.com'],  # Correo al que se enviar√°n las alertas
    'email_on_failure': True,             # Enviar correo si falla la tarea
    'email_on_retry': False,              # No enviar correo en reintentos
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


with DAG(
    'pipeline_spaceflight_news',
    default_args=default_args,
    description='Pipeline orquestado para procesar datos de Spaceflight News',
    schedule_interval='0 6 * * *',  # Corre todos los d√≠as a las 6 AM
    start_date=datetime(2024, 2, 1),
    catchup=False,
) as dag:

    # Tarea 1: Invocar la Cloud Function para la ingesta
    ingesta_task = CloudFunctionInvokeFunctionOperator(
        task_id='invocar_ingesta',
        project_id='mi-proyecto',
        location='us-central1',
        input_data={},
        function_name='ingesta_todos_endpoints'
    )

    # Tarea 2: Ejecutar el job de Dataflow para el procesamiento
    dataflow_task = DataflowCreatePythonJobOperator(
        task_id='procesar_datos_dataflow',
        py_file="gs://mi-bucket/dataflow_pipeline.py",
        job_name='dataflow-pipeline-spaceflight',
        options={
            'project': 'mi-proyecto',
            'region': 'us-central1',
            'streaming': True
        }
    )

    # Tarea 3: Verificaci√≥n de datos en BigQuery
    verificar_datos_task = BigQueryExecuteQueryOperator(
        task_id='verificar_datos',
        sql='SELECT COUNT(*) FROM dataset_noticias.articles WHERE DATE(published_at) = CURRENT_DATE()',
        use_legacy_sql=False
    )

   generar_reporte_task = BigQueryExecuteQueryOperator(
    task_id='generar_reporte_tendencias',
    sql="""
        CREATE OR REPLACE TABLE dataset_noticias.reporte_tendencias AS
        SELECT 
            topic, 
            COUNT(*) AS total_articulos,
            DATE(published_at) AS fecha
        FROM dataset_noticias.articles
        WHERE DATE(published_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
        GROUP BY topic, fecha
        ORDER BY fecha DESC, total_articulos DESC;
    """,
    use_legacy_sql=False
)

    # Definir el flujo de ejecuci√≥n
    ingesta_task >> dataflow_task >> verificar_datos_task >> generar_reporte_task

2Ô∏è‚É£ se sube el archivo al bucket de Cloud Composer:
gsutil cp pipeline_dag.py gs://BUCKET_DE_COMPOSER/dags/


üîπ Explicaci√≥n del C√≥digo
1Ô∏è‚É£ Definici√≥n del DAG:

schedule_interval='0 6 * * *' programa la ejecuci√≥n todos los d√≠as a las 6 AM.
default_args configura par√°metros comunes para todas las tareas (reintentos, retraso entre reintentos, etc.).
2Ô∏è‚É£ CloudFunctionInvokeFunctionOperator: Invoca la Cloud Function para la ingesta de datos.

3Ô∏è‚É£ DataflowCreatePythonJobOperator: Ejecuta el script de Dataflow (dataflow_pipeline.py) que procesa y enriquece los datos.

4Ô∏è‚É£ BigQueryExecuteQueryOperator: Ejecuta una consulta en BigQuery para verificar que los datos fueron procesados correctamente.

5Ô∏è‚É£ Flujo de Ejecuci√≥n (>>): Define las dependencias entre tareas. En este caso:

Primero se ejecuta la ingesta de datos,
Luego se ejecuta el procesamiento en Dataflow,
Finalmente se verifica la existencia de datos en BigQuery.







## IDE de Entendimiento del API
http://190.26.178.21/IDETestGCP/menu.php





